# High-Fidelity Prototype
<iframe style="border: 1px solid rgba(0, 0, 0, 0.1);" width="800" height="450" src="https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FCSHkSKd5450K7fMmgOITnv%2FUGAHacks-Preliminary-Designs%3Fnode-id%3D639%253A218" allowfullscreen></iframe>
[Figma Source File](./images/UGAHacks_HighFidelity.fig)

# Testing Protocol
<h2>Introduction</h2>
<p>At the start of this semester our group set out to solve a specific problem: <q><i>(...) students falling behind on the lesson and in turn becoming more worried on how to follow along as opposed to the material attempting to be taught.</i></q> Following this, we undertook a design process that culminated in the final prototype for this milestone. We believe the choices we made with respect to our design are well informed and adequate for initially solving the problem, and with more time we recognize we would need dedicated user studies (done through A/B testing for example) to evaluate these choices. However, our testing protocol proposal is aimed only at evaluating the effectiveness of our prototype in solving our problem of study and the value of our work.</p>
<h2>Background</h2>
<p>To test our prototype, we are evaluating it in the context of existing literature surrounding computer science education research (<abbr title=Computer Science Education Research>CSRE</abbr>) and cognitive load theory (<abbr title=Cognitive Load Theory>CLT</abbr>). <abbr title=Cognitive Load Theory>CLT</abbr> was proposed by John Sweller initially in the 1980s, and has since been widely accepted as a way to understand information processing. The theory builds of the evolutionary view of human cognitive architecture, which categorizes human knowledge into biologically primary and secondary knowledge. This distinction assumes that humans have evolved to acquire particular types of information, biologically primary knowledge, while other information requires disposition to be learned, biologically secondary knowledge.  Biologically secondary knowledge relies on cognitive processes for knowledge acquisition, organization, and retention, and imposes a working memory load to do so. <abbr title=Cognitive Load Theory>CLT</abbr> identifies two sources of instructional load, intrinsic and extraneous load. The former imposes a heavy cognitive load (<abbr title=Cognitive Load>CL</abbr>) intrinsically and can only be changed by changing what is learned; the latter imposes cognitive load because of how the information is presented and can be changed by altering the instructional procedure. Both types of load are affected by element interactivity, with high element interactivity occurring when learners process a large number of information simultaneously in working memory. Elements of interactivity are attributed to each type of load based on whether they are being processed due to the intrinsic nature of the information or the instructional design, respectively [2].</p>
<p>A recent literature review of <abbr title=Cognitive Load Theory>CLT</abbr> in the context of learning computer programming identifies that most of the existing literature has related <abbr title=Cognitive Load Theory>CLT</abbr> with learning or teaching computer science by studying: the measurement of <abbr title=Cognitive Load>CL</abbr>, instructional design based on <abbr title=Cognitive Load Theory>CLT</abbr> concepts, instructional design based on the development or use of resources or tools, and others. Additionally, the literature review found that for the measurement of <abbr title=Cognitive Load>CL</abbr>, most studies have employed subjective measuring techniques which have proven effective and capable of assessing the mental effort required during learning [1].</p>
<p>With this theoretical basis, our testing protocol is aimed at studying whether our prototype reduces the <abbr title=Cognitive Load>CL</abbr> a student may experience during a computer science lesson, specifically extraneous load. If our test results reflect this, then we will have evidence that our design could effectively solve our problem of study.</p>

<h2>Research Question</h2>
<strong>Does textual representation of text editor commands reduce cognitive load for inexperienced learners in a traditional classroom setting?</strong>

<h2>Methodology</h2>
<p><ul>
	<li>Mixed method approach.</li>
	<li>Within-subjects design.</li>
	<li>User study experiment.</li>
	<li>Use of subjective surveys or interviews.</li>
</ul></p>

<h2>Testing Procedure</h2>
<p>We propose a user study experiment with a within-subjects design, with two experiments that all participants would be part of. Before the experiment, we plan to collect non-identifiable data about each participant regarding their level of experience with Emacs and familiarity with the material that will presented, measured using a Linkert scale. This data will help us determine whether these factors affect our data analysis later on, and is being collected based on our intuition of relevant factors. Each experiment would be a short (15-45 minute) lesson taught as a live-coding lecture on some introductory topic in computer science programming (e.g., classes in object oriented programming) using the Emacs text editor. In one experiment, participants would follow along relying on the instructor's screen and verbal guidance. For the other experiment, participants would use our prototype during the lecture which would give them visual access to the commands the instructor has used to navigate the text editor. After each experiment, we would have participants fill out the computer science cognitive load component survey anonymously, a tool recommended by the literature review for the subjective measurement of cognitive load [4]. Once both experiments are done, we would have a short interview with all participants to collect their feedback and opinions regarding the prototype anonymously, providing us with valuable user insight we expect to need in this initial study.</p>
<p>We choose a live-coding lecture set-up based on the results of a research paper cited in milestone 3, which showed that live-coding reduced <abbr title=Cognitive Load>CL</abbr> when compared to  static code examples [3]. Additionally, a majority of introductory computer science classes taught at UGA are taught using this technique, further supporting our choice. For the duration of the lesson, the times would be chosen to balance the time needed to teach the material and participant fatigue during the study.</p>

<h3>Data analysis</h3>
For the analysis of the results obtained from the computer science cognitive load component survey, the paper cited includes a discussion of how to analyze the results using confirmatory factor analysis after pruning invalid responses (e.g., patterns, all one answer, etc.). For the qualitative data obtained during the interview, we can analyze the data using qualitative content analysis to identify patters in participant feedback and sentiment after the experiment. Finally, for the data collected before the experiment, we can analyze the Linkert scale data by obtaining the mean value and calculating the standard deviation, which can then be used to discuss the impact of these variables on the study.

<h3>Participant Recruitment</h3>
<p>Since our testing procedure has been designed to be done in person and to prove initial viability of our prototype, we would use a convenience sample of students at our university. From this sample pool, we would additionally use case study sampling to specifically choose students who have not taken CSCI 1301 or 1302, which should represent students with little to no experience with programming. From here, all participants would be chosen randomly.</p>

<h3>Informed Consent</h3>
<p>For our study, we should not have any issues in the process of obtaining consent from research participants. Involvement is completely voluntary, and all information regarding the testing procedure can be shared with the participants, that is to say, incomplete disclosure is not necessary for this study. All data collected is anonymous and non-identifiable, in addition to not being sensitive in nature. </p>

<h3> Testing Safely During a Pandemic</h3>
With the recent pandemic of Covid-19 and lingering aftermath, it is prudent to consider how we would plan to conduct our testing procedure to avoid risk of infection. If we were to conduct our testing procedure now, when the vaccine is widely accessible and infection rates are low, we could adapt this procedure to still be done in person. By asking all participants to wear a mask during the experiment and being socially distant as well, the experiment should pose no more risk than in person instruction currently does. If, however, we were in a scenario of peak infection and most public spaces were locked down, we could consider adapting the procedure to be done with virtual participants. The difficulty here would lie in evaluating the impact this would have on the experiment, and therein the value of our findings. 

<h2>Additional Remarks</h2>
<p>In our testing procedure we described a user study experiment that would limit a computer science lesson to a short interval of time as the first step in testing the effect our prototype has on <abbr title=Cognitive Load>CL</abbr>. That said, we recognize this may limit the application of our results when making claims about the effectiveness of our design in the long term as a permanent solution. Future testing could consist of a semester long user study with a between group design, comparing the data of students in different sections of the same programming course using and not using our prototype. Such a study would have it's own challenges to overcome regarding confounding variables related to sample diversity, experience level with text editors, and others, but could provide further insight regarding the effectiveness of our design. In a similar spirit, our initial testing group may prove limiting when we try to make claims about our prototype, and more testing would need to be done with a more random sample of participants.</p>

# Final Summary Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/K5Pd-mo6QUk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<a href="https://youtu.be/K5Pd-mo6QUk">Link to Summary Video</a>

1. J. H. Berssanette and A. C. de Francisco, "Cognitive Load Theory in the Context of Teaching and Learning Computer Programming: A Systematic Literature Review," in IEEE Transactions on Education, doi: 10.1109/TE.2021.3127215.
2. John Sweller, CHAPTER TWO - Cognitive Load Theory, Editor(s): Jose P. Mestre, Brian H. Ross, Psychology of Learning and Motivation, Academic Press, Volume 55, 2011, Pages 37-76, ISSN 0079-7421, ISBN 9780123876911, https://doi.org/10.1016/B978-0-12-387691-1.00002-8. (https://www.sciencedirect.com/science/article/pii/B9780123876911000028)
3. Adalbert Gerald Soosai Raj, Pan Gu, Eda Zhang, Arokia Xavier Annie R, Jim Williams, Richard Halverson, and Jignesh M. Patel. 2020. Live-coding vs Static Code Examples: Which is better with respect to Student Learning and Cognitive Load? In <i>Proceedings of the Twenty-Second Australasian Computing Education Conference</i> (<i>ACE'20</i>). Association for Computing Machinery, New York, NY, USA, 152–159. DOI:https://doi.org/10.1145/3373165.3373182
4. Briana B. Morrison, Brian Dorn, and Mark Guzdial. 2014. Measuring cognitive load in introductory CS: adaptation of an instrument. In <i>Proceedings of the tenth annual conference on International computing education research</i> (<i>ICER '14</i>). Association for Computing Machinery, New York, NY, USA, 131–138. DOI:https://doi.org/10.1145/2632320.2632348