# A. High-Fidelity Prototype
## A1. Figma
<iframe style="border: 1px solid rgba(0, 0, 0, 0.1);" width="800" height="450" src="https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FCSHkSKd5450K7fMmgOITnv%2FUGAHacks-Preliminary-Designs%3Fnode-id%3D639%253A218" allowfullscreen></iframe>
[Figma Source File](./images/UGAHacks_HighFidelity.fig)
[Figma Screenshot](./images/UGAHacks_HighFidelity.jpg)

## A2. User Stories
*As a student who is not familiar with the text editor my professor is using, I want to see the list of commands my professor is typing and an explanation of the keystrokes so I can replicate the action in the same or a different text editor.*

How design decisions address the user story and problem of study:

The application has a settings menu that allows students to toggle between three ways of viewing the commands: the exact command typed by the instructor, the instructor’s command translated to a student’s chosen text editor, and a natural language description of the command. Students can choose any combination of these three options, with between zero and three versions of the commands being displayed at a time. With the different display options, students can view the text editor commands in whatever way is easiest for them. If students want to learn the text editor the instructor is using, they can display the instructor’s commands as well as the natural language commands so they can learn what each command does. If a student is already familiar with a different text editor, they can display the instructor’s commands in terms of the text editor they are already familiar with and can skip using the instructor’s text editor all together. No matter what a student feels most comfortable with, they can easily replicate the actions of the instructor and understand what the instructor’s keystrokes mean. By reducing the amount of learning a student must do in relation to the text editor, that energy can instead be spent on learning the content being taught. 

[Demo Video](https://youtu.be/HAMsTl5iNd8)

*As an instructor teaching an in-person programming class, I want the commands I type to be displayed so that the seating arrangement of my students does not impact their ability to follow my lecture and so they can focus on core concepts instead of text editor commands.*

How design decisions address the user story and problem of study:

Since this application is on each student’s laptop, the seating arrangement and size of the class does not impact the use of the application and viewing instructor commands. The application lists the last seven commands typed by the instructor for the students to view. Seven commands are listed because people are only able to remember 7 plus or minus 2 chunks of information at a time according to Shneiderman et.al (2017).  By providing students with access to the last seven commands, they do not have to remember these chunks of information, and can instead focus on the material being discussed in class. Reducing the cognitive load related to using a text editor allows for a deeper understanding of the material and less distraction. In case there are students who get behind or confused, there is a scroll bar on the right side of the application so that students may scroll up and find the commands they missed. Students do not have to attempt to figure out what commands they missed or distract the class by asking questions, but can instead review the list of commands and where they went wrong. 

[Demo Video](https://youtu.be/11gggA2KSwc)

*As a student who is new to programming, I want to see the list of commands that my professor types into the text editor so that I can focus on the concepts being taught instead of memorizing keystrokes or falling behind in class.*

How design decisions address the user story and problem of study:

In addition to listing the last seven commands, which is addressed in a previous user story design, the application also allows students to flag and make notes on any command typed by the instructor. If a student is confused about what a command should do, or a student begins to fall behind at a specific command, they can flag that command. Students can then easily return to the command later and instructors are notified that a student flagged the command. If many students flag a certain command, instructors are able to easily identify that there is a problem and can address it. Allowing students to flag commands provides students with a way to quickly mark problems and move on so that they do not fall behind. Students are also able to make notes on commands. Similar to flagging a command, students can easily return to these notes and review what they wrote at a later time. Whether the note pertains to the command itself or the content being discussed in the class, students can quickly type notes without falling any further behind in class. 

[Demo Video](https://youtu.be/jPaDvTPEThg)

*As a student who has difficulties following along in class (due to visual or learning disabilities), I want the list of the commands my professor typed transposed into a format that I can access.*

How design decisions address the user story and problem of study:

The application provides students with three options of how the application is displayed: light mode, dark mode, and color mode. Since many computer science students prefer dark mode, this is the default display for our application. Students can easily switch between color modes under the color menu. All three options are color blind accessible; each was put through a color blind simulator to ensure this. The color mode of the application was created and checked by Adobe’s color palette generator. Students are able to view the application in a way that is accessible despite potential color blind challenges. 

[Demo Video](https://youtu.be/oViyKQ4hz8k)


# Testing Protocol
<h2>Introduction</h2>
<p>At the start of this semester our group set out to solve a specific problem: <q><i>(...) students falling behind on the lesson and in turn becoming more worried on how to follow along as opposed to the material attempting to be taught.</i></q> Following this, we undertook a design process that culminated in the final prototype for this milestone. We believe the choices we made with respect to our design are well informed and adequate for initially solving the problem, and with more time we recognize we would need dedicated user studies (done through A/B testing for example) to evaluate these choices. However, our testing protocol proposal is aimed only at evaluating the effectiveness of our prototype in solving our problem of study and the value of our work.</p>
<h2>Background</h2>
<p>To test our prototype, we are evaluating it in the context of existing literature surrounding computer science education research (<abbr title=Computer Science Education Research>CSRE</abbr>) and cognitive load theory (<abbr title=Cognitive Load Theory>CLT</abbr>). <abbr title=Cognitive Load Theory>CLT</abbr> was proposed by John Sweller initially in the 1980s, and has since been widely accepted as a way to understand information processing. The theory builds of the evolutionary view of human cognitive architecture, which categorizes human knowledge into biologically primary and secondary knowledge. This distinction assumes that humans have evolved to acquire particular types of information, biologically primary knowledge, while other information requires disposition to be learned, biologically secondary knowledge.  Biologically secondary knowledge relies on cognitive processes for knowledge acquisition, organization, and retention, and imposes a working memory load to do so. <abbr title=Cognitive Load Theory>CLT</abbr> identifies two sources of instructional load, intrinsic and extraneous load. The former imposes a heavy cognitive load (<abbr title=Cognitive Load>CL</abbr>) intrinsically and can only be changed by changing what is learned; the latter imposes cognitive load because of how the information is presented and can be changed by altering the instructional procedure. Both types of load are affected by element interactivity, with high element interactivity occurring when learners process a large number of information simultaneously in working memory. Elements of interactivity are attributed to each type of load based on whether they are being processed due to the intrinsic nature of the information or the instructional design, respectively [2].</p>
<p>A recent literature review of <abbr title=Cognitive Load Theory>CLT</abbr> in the context of learning computer programming identifies that most of the existing literature has related <abbr title=Cognitive Load Theory>CLT</abbr> with learning or teaching computer science by studying: the measurement of <abbr title=Cognitive Load>CL</abbr>, instructional design based on <abbr title=Cognitive Load Theory>CLT</abbr> concepts, instructional design based on the development or use of resources or tools, and others. Additionally, the literature review found that for the measurement of <abbr title=Cognitive Load>CL</abbr>, most studies have employed subjective measuring techniques which have proven effective and capable of assessing the mental effort required during learning [1].</p>
<p>With this theoretical basis, our testing protocol is aimed at studying whether our prototype reduces the <abbr title=Cognitive Load>CL</abbr> a student may experience during a computer science lesson, specifically extraneous load. If our test results reflect this, then we will have evidence that our design could effectively solve our problem of study.</p>

<h2>Research Question</h2>
<strong>Does textual representation of text editor commands reduce cognitive load for inexperienced learners in a traditional classroom setting?</strong>

<h2>Methodology</h2>
<p><ul>
	<li>Mixed method approach.</li>
	<li>Within-subjects design.</li>
	<li>User study experiment.</li>
	<li>Use of subjective surveys or interviews.</li>
</ul></p>

<h2>Testing Procedure</h2>
<p>We propose a user study experiment with a within-subjects design, with two experiments that all participants would be part of. Before the experiment, we plan to collect non-identifiable data about each participant regarding their level of experience with Emacs and familiarity with the material that will presented, measured using a Linkert scale. This data will help us determine whether these factors affect our data analysis later on, and is being collected based on our intuition of relevant factors. Each experiment would be a short (15-45 minute) lesson taught as a live-coding lecture on some introductory topic in computer science programming (e.g., classes in object oriented programming) using the Emacs text editor. In one experiment, participants would follow along relying on the instructor's screen and verbal guidance. For the other experiment, participants would use our prototype during the lecture which would give them visual access to the commands the instructor has used to navigate the text editor. After each experiment, we would have participants fill out the computer science cognitive load component survey anonymously, a tool recommended by the literature review for the subjective measurement of cognitive load [4]. Once both experiments are done, we would have a short interview with all participants to collect their feedback and opinions regarding the prototype anonymously, providing us with valuable user insight we expect to need in this initial study.</p>
<p>We choose a live-coding lecture set-up based on the results of a research paper cited in milestone 3, which showed that live-coding reduced <abbr title=Cognitive Load>CL</abbr> when compared to  static code examples [3]. Additionally, a majority of introductory computer science classes taught at UGA are taught using this technique, further supporting our choice. For the duration of the lesson, the times would be chosen to balance the time needed to teach the material and participant fatigue during the study.</p>

<h3>Data analysis</h3>
For the analysis of the results obtained from the computer science cognitive load component survey, the paper cited includes a discussion of how to analyze the results using confirmatory factor analysis after pruning invalid responses (e.g., patterns, all one answer, etc.). For the qualitative data obtained during the interview, we can analyze the data using qualitative content analysis to identify patters in participant feedback and sentiment after the experiment. Finally, for the data collected before the experiment, we can analyze the Linkert scale data by obtaining the mean value and calculating the standard deviation, which can then be used to discuss the impact of these variables on the study.

<h3>Participant Recruitment</h3>
<p>Since our testing procedure has been designed to be done in person and to prove initial viability of our prototype, we would use a convenience sample of students at our university. From this sample pool, we would additionally use case study sampling to specifically choose students who have not taken CSCI 1301 or 1302, which should represent students with little to no experience with programming. From here, all participants would be chosen randomly.</p>

<h3>Informed Consent</h3>
<p>For our study, we should not have any issues in the process of obtaining consent from research participants. Involvement is completely voluntary, and all information regarding the testing procedure can be shared with the participants, that is to say, incomplete disclosure is not necessary for this study. All data collected is anonymous and non-identifiable, in addition to not being sensitive in nature. </p>

<h3> Testing Safely During a Pandemic</h3>
With the recent pandemic of Covid-19 and lingering aftermath, it is prudent to consider how we would plan to conduct our testing procedure to avoid risk of infection. If we were to conduct our testing procedure now, when the vaccine is widely accessible and infection rates are low, we could adapt this procedure to still be done in person. By asking all participants to wear a mask during the experiment and being socially distant as well, the experiment should pose no more risk than in person instruction currently does. If, however, we were in a scenario of peak infection and most public spaces were locked down, we could consider adapting the procedure to be done with virtual participants. The difficulty here would lie in evaluating the impact this would have on the experiment, and therein the value of our findings. 

<h2>Additional Remarks</h2>
<p>In our testing procedure we described a user study experiment that would limit a computer science lesson to a short interval of time as the first step in testing the effect our prototype has on <abbr title=Cognitive Load>CL</abbr>. That said, we recognize this may limit the application of our results when making claims about the effectiveness of our design in the long term as a permanent solution. Future testing could consist of a semester long user study with a between group design, comparing the data of students in different sections of the same programming course using and not using our prototype. Such a study would have it's own challenges to overcome regarding confounding variables related to sample diversity, experience level with text editors, and others, but could provide further insight regarding the effectiveness of our design. In a similar spirit, our initial testing group may prove limiting when we try to make claims about our prototype, and more testing would need to be done with a more random sample of participants.</p>

# Final Summary Video
<iframe width="560" height="315" src="https://www.youtube.com/embed/K5Pd-mo6QUk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<a href="https://youtu.be/K5Pd-mo6QUk">Link to Summary Video</a>

1. J. H. Berssanette and A. C. de Francisco, "Cognitive Load Theory in the Context of Teaching and Learning Computer Programming: A Systematic Literature Review," in IEEE Transactions on Education, doi: 10.1109/TE.2021.3127215.
2. John Sweller, CHAPTER TWO - Cognitive Load Theory, Editor(s): Jose P. Mestre, Brian H. Ross, Psychology of Learning and Motivation, Academic Press, Volume 55, 2011, Pages 37-76, ISSN 0079-7421, ISBN 9780123876911, https://doi.org/10.1016/B978-0-12-387691-1.00002-8. (https://www.sciencedirect.com/science/article/pii/B9780123876911000028)
3. Adalbert Gerald Soosai Raj, Pan Gu, Eda Zhang, Arokia Xavier Annie R, Jim Williams, Richard Halverson, and Jignesh M. Patel. 2020. Live-coding vs Static Code Examples: Which is better with respect to Student Learning and Cognitive Load? In <i>Proceedings of the Twenty-Second Australasian Computing Education Conference</i> (<i>ACE'20</i>). Association for Computing Machinery, New York, NY, USA, 152–159. DOI:https://doi.org/10.1145/3373165.3373182
4. Briana B. Morrison, Brian Dorn, and Mark Guzdial. 2014. Measuring cognitive load in introductory CS: adaptation of an instrument. In <i>Proceedings of the tenth annual conference on International computing education research</i> (<i>ICER '14</i>). Association for Computing Machinery, New York, NY, USA, 131–138. DOI:https://doi.org/10.1145/2632320.2632348